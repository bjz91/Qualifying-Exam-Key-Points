\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[parfill]{parskip} % new line between paragraphs, no indentation
\usepackage[colorlinks,pdfstartview=FitH,citecolor=blue]{hyperref}
\usepackage{xeCJK} % Enabling Chinese characters
\usepackage{fancyvrb} % In-line verbatim; for "\Verb" macro

% Header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}%Clear all heads and foots
\setlength{\headheight}{35pt} %Eliminate the warning of "headheight is too samll"
\rhead{Qualifying Exam Key Points\\Jianzhao Bi\\\today}
\cfoot{\thepage}

\begin{document}

\section{Aim 2}
\subsection{Key Points}
\begin{itemize}
    \item Why using KED model instead of other interpolation? {
    
    There are many other interpolation methods can be tested (such as the variational approach, that it, splines). It is necessary to check the performance of the methods, such as the preservation of geometrical properties, and then determine the final method.
    }
\end{itemize}

\section{Aim 3}

\subsection{Key Points}
\begin{itemize}
    \item Routine measurements made by local and federal monitoring programs are generally available only every 3 or 6 days, which limits their usefulness for studies of associations between health outcomes and daily variations in pollutant concentrations \citep{sarnat2015fine}.
    \item Pollutants with greater measurement error are likely to exhibit weaker associations with health outcomes than pollutants with less error, even if they are not inherently less toxic.
    \item Why only choose these PM2.5 components species? {
        \begin{itemize}
            \item We selected species that represented different chemical component classes, which may plausibly confer different toxicities based on different chemical properties \citep{suh2011chemical}. 
            \item Consideration was also given to species associated with health outcomes in previous studies \citep{chen2009effects, kelly2012size, rohr2012attributing}.
            \item The concentrations of the species should be relatively high, with a small samples below the detection limit (BDL) (BDL generally $<5\%$).
        \end{itemize}
    }
    \item Covariates in the model {
        \begin{itemize}
            \item Indicator variables to control for season (\textit{i.e.,} fall, winter, spring, and summer)
            \item Day of week, holidays
            \item Time trends using cubic splines for day of visit with monthly knots (the base trend of the ED visits)
            \item Temperature: using cubic splines for lag 0 maximum temperature with knots placed at the 25$^{th}$ and 75$^{th}$ percentiles, cubic terms for 1 to 2 day moving-average minimum temperature, and cubic terms for 0 to 2 day moving-average dew point temperature \citep{strickland2010short}.
            \item \textcolor{red}{Cancer, cardiovascular disease, chronic lung disease, diabetes, hyperlipidemia (高血脂), hypertension (高血压)}
        \end{itemize}
    }
    \item Sensitivity analysis {
        \begin{enumerate}
            \item Misspecification and the potential for residual confounding by temporal factors: estimating associations with pollutant concentrations on the day after the emergency department visit (lag $-1$) given pollutant levels on the days of interest \citep{flanders2011method}. These lag $-1$ associations are assumed to reflect noncausal mechanisms of association because the exposures occurred after the outcome, suggesting the possibility of some model misspecification and/or residual confounding in primary models assessing the effects of these pollutants \citep{flanders2011method}.
            \item Alternate model specifications: 1) alternate time trend control (cubic spline for day of visit with two knots per month and one knot every 2 months, respectively, instead of one knot per month); 2) alternate temperature control (indicator variables for each degree Celsius instead of a cubic spline for lag 0 maximum temperature).
            \item Assess the robustness of our results to lag structure: examined 5-day distributed lag models (lags 0--4), with control for minimum and dew point temperature adjusted to include the moving average of lags 1--4 and 0--4, respectively.
            \item Potential for confounding of selected single-pollutant results: co-pollutants using two-pollutant models
        \end{enumerate}
    }
    \item How to estimate the burden of kidney disease caused by PM2.5 exposure? {
        \begin{itemize}
            \item Population attributable fraction (PAF) represents the proportional reduction in population disease that would occur if exposure to PM2.5 was reduced to the Environmental Protection Agency's (EPA) recommended levels of 12 $mg/m^3$ \citep{bowe2018particulate}.
            \item \citet{bowe2018particulate} estimated the national burden of CKD attributable to elevated levels of PM2.5 exceeding the EPA standard (where the theoretical minimum risk exposure level [TMREL] was set at the EPA standard of 12 $mg/m^3$) in the contiguous United States was 44,793 incident cases per year (95\% uncertainty interval [95\% UI], 42,716 to 46,869). The national burden of ESRD attributable to PM2.5 levels in excess of EPA standards was 2438 incident cases per year (95\% UI, 1963 to 2902). 
        \end{itemize}
    }
    \item \href{https://rdrr.io/cran/powerMediation/man/sizePoisson.html}{Sample size} calculation for simple Poisson regression. Assume the predictor is normally distributed. {
       
        Take PM2.5 as an example: the mean concentration is $15\;\mu g/m^3$; the standard deviation is $7\;\mu g/m^3$; the IQR is $9\;\mu g/m^3$; the expected rate ratio is 1.035; Then, according to ``\Verb+sizePossion+'', the minimum sample size is 8741 counts at 5\% significance level and the power of 0.8. If the average daily counts for ARF is 10 $person/day$, there should be 874 days' data ($\sim 2.4\;years$). 
         \begin{verbatim}
            library(powerMediation)
            sizePoisson(beta0 = 0, beta1 = log(1.035)/9, mu.x1 = 15, 
                sigma2.x1 = 7*7, mu.T = 1, phi = 1, alpha = 0.1, power = 0.8)
        \end{verbatim}
    }
    \fbox{
        \parbox{0.9\textwidth}{
            The simple Poisson regression has the following form:
            \begin{equation*}
                Pr(Y_i = y_i | \mu_i, t_i) = \exp(-\mu_i t_i) (\mu_i t_i)^{y_i}/ (y_i!)
            \end{equation*}
            where
            \begin{equation*}
                \mu_i=\exp(\beta_0+\beta_1 x_{1i})
            \end{equation*}
            We are interested in testing the null hypothesis $\beta_1=0$ versus the alternative hypothesis $\beta_1=\theta_1$. Assume $x_1$ is normally distributed with mean $\mu_{x_1}$ and variance $\sigma^2_{x_1}$. The sample size calculation formula derived by \citet{signorini1991sample} is
            \begin{equation*}
                N=\phi{[z_{1-\alpha/2}\sqrt{V(b_1 | \beta_1=0)} +z_{power}\sqrt{V(b_1 | \beta_1=\theta_1)}]^2}/ {\mu_T \exp(\beta_0) \theta_1^2}
            \end{equation*}
            where $\phi$ is the over-dispersion parameter $(var(y_i)/mean(y_i))$, $\alpha$ is the type I error rate, $b_1$ is the estimate of the slope $\beta_1$, $\beta_0$ is the intercept, $\mu_T$ is the mean exposure time, $z_a$ is the $100\times\alpha^{th}$ lower percentile of the standard normal distribution, and $V(b_1|\beta_1=\theta)$ is the variance of the estimate $b_1$ given the true slope $\beta_1=\theta$.
            
            The variances are
            \begin{equation*}
                V(b_1 | \beta_1 = 0)=1/{\sigma^2_{x_1}}
            \end{equation*}
            and
            \begin{equation*}
                V(b_1 | \beta_1 = \theta_1)=1/{\sigma^2_{x_1}} \exp[-(\theta_1 \mu_{x_1} + \theta_1^2\sigma^2_{x_1}/2)]
            \end{equation*}
        }
    }
\end{itemize}

\subsection{Statistical Knowledge}
\begin{itemize}
    \item Standardization is the necessary step for the hypothesis testing, that is, to transform a specific distribution to its standard distribution (\textit{e.g.,} standard normal distribution $z=\frac{X-\mu}{\sigma}$). By doing this, the probability can be easily calculated. 
    \item IQR is more robust (less sensitive) than Range. IQR is not as sensitive to shape of distribution or to extreme values (outliers).
    \item Confidence Interval (CI): a level $C=1-\alpha$ confidence interval for a parameter is an interval computed from sample data by a method that has probability $C$ of producing an interval containing the true value of the parameter.
        \begin{itemize}
            \item The observed interval brackets the true value of $\mu$, with confidence $100(1-\alpha)\%$; that is the procedure successfully yields a CI that captures the true value $100(1-\alpha)\%$ of the time. 
            \item With repeated sampling, $100(1-\alpha)\%$ of the intervals formed using this procedure will capture the true value of $\mu$.
        \end{itemize}
         \begin{align*}
                   &P[-z_{\alpha/2}\leq \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}]=1-\alpha \\
        \therefore &P[\bar{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq \mu \leq \bar{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}]=1-\alpha
        \end{align*}
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}